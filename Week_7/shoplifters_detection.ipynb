{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KtEgjhTuNAhV",
        "outputId": "dfa7ff2d-be85-40eb-a647-57cf89b9f265"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "998XoEyJaw1Q"
      },
      "source": [
        "# libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "59KvOsFVZu_p"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, LSTM, Conv2D, MaxPooling2D, Flatten, TimeDistributed, GlobalAveragePooling2D, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.utils import to_categorical, Sequence\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "import glob\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import gc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "eUBGwgVmZw6N"
      },
      "outputs": [],
      "source": [
        "# Set memory growth for GPU\n",
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "if len(physical_devices) > 0:\n",
        "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "\n",
        "# Configuration parameters\n",
        "CONFIG = {\n",
        "    'seed': 42,\n",
        "    'target_size': (224, 224),  # Resizing frames to save memory\n",
        "    'batch_size': 4,  # Small batch size to avoid memory issues\n",
        "    'epochs': 10,\n",
        "    'frames_per_video': 16,  # Taking subset of frames to save memory\n",
        "    'learning_rate': 1e-4,\n",
        "    'num_classes': 2,  # Shoplifter vs Non-shoplifter\n",
        "}\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "tf.random.set_seed(CONFIG['seed'])\n",
        "np.random.seed(CONFIG['seed'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Y1IPud6EZw3F"
      },
      "outputs": [],
      "source": [
        "# Create a DataFrame with video paths and labels\n",
        "def create_dataframe(shoplifters_dir, non_shoplifters_dir):\n",
        "    shoplifters_paths = glob.glob(os.path.join(shoplifters_dir, '*.mp4'))\n",
        "    non_shoplifters_paths = glob.glob(os.path.join(non_shoplifters_dir, '*.mp4'))\n",
        "\n",
        "    shoplifters_df = pd.DataFrame({\n",
        "        'path': shoplifters_paths,\n",
        "        'label': 1  # 1 for shoplifter\n",
        "    })\n",
        "\n",
        "    non_shoplifters_df = pd.DataFrame({\n",
        "        'path': non_shoplifters_paths,\n",
        "        'label': 0  # 0 for non-shoplifter\n",
        "    })\n",
        "\n",
        "    df = pd.concat([shoplifters_df, non_shoplifters_df], ignore_index=True)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "CozNNnZ3Zw0d"
      },
      "outputs": [],
      "source": [
        "# Exploratory Data Analysis\n",
        "def perform_eda(df, shoplifters_dir, non_shoplifters_dir):\n",
        "    print(\"Data Summary:\")\n",
        "    print(f\"Total videos: {len(df)}\")\n",
        "    print(f\"Shoplifter videos: {len(df[df['label'] == 1])}\")\n",
        "    print(f\"Non-shoplifter videos: {len(df[df['label'] == 0])}\")\n",
        "\n",
        "    # Sample a few videos to analyze\n",
        "    sample_videos = df.sample(min(10, len(df)), random_state=CONFIG['seed'])\n",
        "\n",
        "    video_stats = []\n",
        "    for _, row in sample_videos.iterrows():\n",
        "        video_path = row['path']\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "        # Get video properties\n",
        "        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "        duration = frame_count / fps\n",
        "\n",
        "        video_stats.append({\n",
        "            'path': video_path,\n",
        "            'label': row['label'],\n",
        "            'frame_count': frame_count,\n",
        "            'fps': fps,\n",
        "            'width': width,\n",
        "            'height': height,\n",
        "            'duration': duration\n",
        "        })\n",
        "\n",
        "        cap.release()\n",
        "\n",
        "    stats_df = pd.DataFrame(video_stats)\n",
        "    print(\"\\nVideo Statistics:\")\n",
        "    print(stats_df.describe())\n",
        "\n",
        "    # Plot distribution of frame counts\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.histplot(data=stats_df, x='frame_count', hue='label',\n",
        "                 element='step', common_norm=False, bins=20)\n",
        "    plt.title('Distribution of Frame Counts')\n",
        "    plt.xlabel('Number of Frames')\n",
        "    plt.ylabel('Count')\n",
        "    plt.legend(['Non-Shoplifter', 'Shoplifter'])\n",
        "    plt.savefig('frame_count_distribution.png')\n",
        "    plt.close()\n",
        "\n",
        "    # Plot distribution of video durations\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.histplot(data=stats_df, x='duration', hue='label',\n",
        "                 element='step', common_norm=False, bins=20)\n",
        "    plt.title('Distribution of Video Durations')\n",
        "    plt.xlabel('Duration (seconds)')\n",
        "    plt.ylabel('Count')\n",
        "    plt.legend(['Non-Shoplifter', 'Shoplifter'])\n",
        "    plt.savefig('duration_distribution.png')\n",
        "    plt.close()\n",
        "\n",
        "    return stats_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Zo7WoGrpZwyg"
      },
      "outputs": [],
      "source": [
        "# Video Data Generator\n",
        "class VideoDataGenerator(Sequence):\n",
        "    def __init__(self, dataframe, batch_size, frames_per_video, target_size, num_classes, shuffle=True):\n",
        "        self.dataframe = dataframe\n",
        "        self.batch_size = batch_size\n",
        "        self.frames_per_video = frames_per_video\n",
        "        self.target_size = target_size\n",
        "        self.num_classes = num_classes\n",
        "        self.shuffle = shuffle\n",
        "        self.indexes = np.arange(len(dataframe))\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "        # Image data augmentation\n",
        "        self.img_gen = ImageDataGenerator(\n",
        "            rescale=1./255,\n",
        "            rotation_range=10,\n",
        "            width_shift_range=0.1,\n",
        "            height_shift_range=0.1,\n",
        "            shear_range=0.1,\n",
        "            zoom_range=0.1,\n",
        "            horizontal_flip=True,\n",
        "            fill_mode='nearest'\n",
        "        )\n",
        "\n",
        "        # Handle empty batches issue by preprocessing to identify valid videos\n",
        "        self.valid_indices = []\n",
        "        for i in range(len(self.dataframe)):\n",
        "            video_path = self.dataframe.iloc[i]['path']\n",
        "            if self._check_video_valid(video_path):\n",
        "                self.valid_indices.append(i)\n",
        "\n",
        "        self.indexes = np.array(self.valid_indices)\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "    def _check_video_valid(self, video_path):\n",
        "        \"\"\"Check if video has enough frames to extract.\"\"\"\n",
        "        try:\n",
        "            cap = cv2.VideoCapture(video_path)\n",
        "            frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "            cap.release()\n",
        "            return frame_count >= self.frames_per_video\n",
        "        except:\n",
        "            return False\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Denotes the number of batches per epoch\"\"\"\n",
        "        return int(np.ceil(len(self.indexes) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Generate one batch of data\"\"\"\n",
        "        # Generate indexes of the batch\n",
        "        batch_indexes = self.indexes[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "\n",
        "        # Find list of IDs\n",
        "        batch_df = self.dataframe.iloc[batch_indexes]\n",
        "\n",
        "        batch_videos = []\n",
        "        batch_labels = []\n",
        "\n",
        "        for _, row in batch_df.iterrows():\n",
        "            video_path = row['path']\n",
        "            label = row['label']\n",
        "\n",
        "            # Process video\n",
        "            frames = self._extract_frames(video_path)\n",
        "            if frames is not None and len(frames) == self.frames_per_video:\n",
        "                batch_videos.append(frames)\n",
        "                batch_labels.append(label)\n",
        "\n",
        "        # Ensure we have at least one valid video in the batch\n",
        "        if len(batch_videos) == 0:\n",
        "            # If no valid videos in batch, use first valid video\n",
        "            first_valid_path = self.dataframe.iloc[self.indexes[0]]['path']\n",
        "            first_valid_label = self.dataframe.iloc[self.indexes[0]]['label']\n",
        "            frames = self._extract_frames(first_valid_path)\n",
        "            if frames is not None:\n",
        "                batch_videos.append(frames)\n",
        "                batch_labels.append(first_valid_label)\n",
        "\n",
        "        # Convert to numpy arrays\n",
        "        batch_videos = np.array(batch_videos)\n",
        "        batch_labels = to_categorical(np.array(batch_labels), num_classes=self.num_classes)\n",
        "\n",
        "        return batch_videos, batch_labels\n",
        "\n",
        "    def _extract_frames(self, video_path):\n",
        "        try:\n",
        "            cap = cv2.VideoCapture(video_path)\n",
        "            frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "            if frame_count <= 0:\n",
        "                cap.release()\n",
        "                return None\n",
        "\n",
        "            # Calculate indices of frames to extract\n",
        "            indices = np.linspace(0, frame_count - 1, self.frames_per_video, dtype=int)\n",
        "\n",
        "            frames = []\n",
        "            for i in indices:\n",
        "                cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
        "                ret, frame = cap.read()\n",
        "\n",
        "                if not ret:\n",
        "                    continue\n",
        "\n",
        "                # Convert BGR to RGB\n",
        "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "                # Resize frame\n",
        "                frame = cv2.resize(frame, self.target_size)\n",
        "\n",
        "                # Apply image augmentation\n",
        "                frame = self.img_gen.random_transform(frame)\n",
        "\n",
        "                # Preprocess for MobileNetV2 (normalize to [-1, 1])\n",
        "                frame = frame / 127.5 - 1.0\n",
        "\n",
        "                frames.append(frame)\n",
        "\n",
        "            cap.release()\n",
        "\n",
        "            # If we couldn't extract enough frames, return None\n",
        "            if len(frames) < self.frames_per_video:\n",
        "                return None\n",
        "\n",
        "            return np.array(frames)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing video {video_path}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"Updates indexes after each epoch\"\"\"\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indexes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhFPIE3MgbOd"
      },
      "source": [
        "# Modify the create_model Function\n",
        "We’ll use MobileNetV2 as the pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "WARQZ-XcaJMy"
      },
      "outputs": [],
      "source": [
        "def create_model(input_shape, num_classes):\n",
        "    \"\"\"\n",
        "    Create an improved video classification model for shoplifting detection\n",
        "    with class imbalance handling\n",
        "\n",
        "    Parameters:\n",
        "    - input_shape: Shape of input data (frames, height, width, channels)\n",
        "    - num_classes: Number of output classes\n",
        "\n",
        "    Returns:\n",
        "    - Compiled Keras model\n",
        "    \"\"\"\n",
        "    # Import required layers\n",
        "    from tensorflow.keras.layers import Conv2D, BatchNormalization, MaxPooling2D, Flatten\n",
        "    from tensorflow.keras.layers import Dense, Dropout, LSTM, TimeDistributed, Input\n",
        "    from tensorflow.keras.layers import GlobalAveragePooling2D, Bidirectional\n",
        "    from tensorflow.keras.models import Model\n",
        "    from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "    # Input shape\n",
        "    input_layer = Input(shape=input_shape)\n",
        "\n",
        "    # Load MobileNetV2 as the base model (pretrained on ImageNet)\n",
        "    base_model = MobileNetV2(\n",
        "        weights='imagenet',  # Use pretrained weights\n",
        "        include_top=False,    # Exclude the classification head\n",
        "        input_shape=input_shape[1:]  # Shape of each frame (height, width, channels)\n",
        "    )\n",
        "\n",
        "    # Freeze the base model initially\n",
        "    base_model.trainable = False\n",
        "\n",
        "\n",
        "   # Wrap the base model in TimeDistributed to process each frame\n",
        "    x = TimeDistributed(base_model)(input_layer)\n",
        "\n",
        "    # Use GlobalAveragePooling2D to reduce spatial dimensions\n",
        "    x = TimeDistributed(GlobalAveragePooling2D())(x)\n",
        "\n",
        "    # Bidirectional LSTM for temporal modeling\n",
        "    x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
        "    x = Dropout(0.4)(x)\n",
        "    x = Bidirectional(LSTM(64))(x)\n",
        "    x = Dropout(0.4)(x)\n",
        "\n",
        "    # Dense layers for classification\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "\n",
        "    # Output layer\n",
        "    output = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    # Create the final model\n",
        "    model = Model(inputs=input_layer, outputs=output)\n",
        "\n",
        "    # Compile model with class weighting considerations\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=1e-4),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy',\n",
        "                 tf.keras.metrics.Precision(name='precision'),\n",
        "                 tf.keras.metrics.Recall(name='recall'),\n",
        "                 tf.keras.metrics.AUC(name='auc')]\n",
        "    )\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHGLDVGoaJJ0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "OiNhUFQ2aJG8"
      },
      "outputs": [],
      "source": [
        "# Function to evaluate the model\n",
        "def evaluate_model(model, test_gen):\n",
        "    # Initialize arrays for predictions and ground truth\n",
        "    all_predictions = []\n",
        "    all_true_labels = []\n",
        "\n",
        "    # Loop through the test generator\n",
        "    for i in range(len(test_gen)):\n",
        "        x, y = test_gen[i]\n",
        "\n",
        "        # Get model predictions\n",
        "        pred = model.predict(x)\n",
        "\n",
        "        # Store predictions and true labels\n",
        "        all_predictions.extend(np.argmax(pred, axis=1))\n",
        "        all_true_labels.extend(np.argmax(y, axis=1))\n",
        "\n",
        "    # Calculate metrics\n",
        "    from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "    # Accuracy\n",
        "    acc = accuracy_score(all_true_labels, all_predictions)\n",
        "    print(f\"\\nTest Accuracy: {acc:.4f}\")\n",
        "\n",
        "    # Classification report\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(all_true_labels, all_predictions, target_names=['Non-Shoplifter', 'Shoplifter']))\n",
        "\n",
        "    # Confusion matrix\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    cm = confusion_matrix(all_true_labels, all_predictions)\n",
        "    print(cm)\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=['Non-Shoplifter', 'Shoplifter'],\n",
        "                yticklabels=['Non-Shoplifter', 'Shoplifter'])\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.savefig('confusion_matrix.png')\n",
        "    plt.close()\n",
        "\n",
        "    return all_true_labels, all_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "X-3aRhw8aUZ1"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    # Set paths to your data directories\n",
        "    shoplifters_dir = '/content/drive/MyDrive/Computer_Vision_intern/week_5/Shop_DataSet/shop lifters'\n",
        "    non_shoplifters_dir = '/content/drive/MyDrive/Computer_Vision_intern/week_5/Shop_DataSet/non shop lifters'\n",
        "\n",
        "    # Create dataframe\n",
        "    df = create_dataframe(shoplifters_dir, non_shoplifters_dir)\n",
        "\n",
        "    # Split the data\n",
        "    train_df, temp_df = train_test_split(df, test_size=0.4, random_state=CONFIG['seed'], stratify=df['label'])\n",
        "    val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=CONFIG['seed'], stratify=temp_df['label'])\n",
        "\n",
        "    print(f\"Train set: {len(train_df)} videos\")\n",
        "    print(f\"- Shoplifters: {len(train_df[train_df['label'] == 1])}\")\n",
        "    print(f\"- Non-shoplifters: {len(train_df[train_df['label'] == 0])}\")\n",
        "\n",
        "    print(f\"Validation set: {len(val_df)} videos\")\n",
        "    print(f\"- Shoplifters: {len(val_df[val_df['label'] == 1])}\")\n",
        "    print(f\"- Non-shoplifters: {len(val_df[val_df['label'] == 0])}\")\n",
        "\n",
        "    print(f\"Test set: {len(test_df)} videos\")\n",
        "    print(f\"- Shoplifters: {len(test_df[test_df['label'] == 1])}\")\n",
        "    print(f\"- Non-shoplifters: {len(test_df[test_df['label'] == 0])}\")\n",
        "\n",
        "    # Calculate class weights to handle imbalance\n",
        "    total = len(df)\n",
        "    n_shoplifters = len(df[df['label'] == 1])\n",
        "    n_non_shoplifters = len(df[df['label'] == 0])\n",
        "\n",
        "    class_weight = {\n",
        "        0: total / (2.0 * n_non_shoplifters),  # Weight for non-shoplifters\n",
        "        1: total / (2.0 * n_shoplifters)       # Weight for shoplifters\n",
        "    }\n",
        "    print(f\"Using class weights: {class_weight}\")\n",
        "\n",
        "    # Create data generators\n",
        "    train_gen = VideoDataGenerator(\n",
        "        train_df,\n",
        "        batch_size=CONFIG['batch_size'],\n",
        "        frames_per_video=CONFIG['frames_per_video'],\n",
        "        target_size=CONFIG['target_size'],\n",
        "        num_classes=CONFIG['num_classes'],\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    val_gen = VideoDataGenerator(\n",
        "        val_df,\n",
        "        batch_size=CONFIG['batch_size'],\n",
        "        frames_per_video=CONFIG['frames_per_video'],\n",
        "        target_size=CONFIG['target_size'],\n",
        "        num_classes=CONFIG['num_classes'],\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    test_gen = VideoDataGenerator(\n",
        "        test_df,\n",
        "        batch_size=CONFIG['batch_size'],\n",
        "        frames_per_video=CONFIG['frames_per_video'],\n",
        "        target_size=CONFIG['target_size'],\n",
        "        num_classes=CONFIG['num_classes'],\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    # Create the model\n",
        "    input_shape = (CONFIG['frames_per_video'], CONFIG['target_size'][0], CONFIG['target_size'][1], 3)\n",
        "    model = create_model(input_shape, CONFIG['num_classes'])\n",
        "\n",
        "    # Print model summary\n",
        "    model.summary()\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(\n",
        "        train_gen,\n",
        "        validation_data=val_gen,\n",
        "        epochs=CONFIG['epochs'],\n",
        "        class_weight=class_weight,\n",
        "        callbacks=[\n",
        "            ModelCheckpoint('best_model.keras', monitor='val_auc', save_best_only=True, mode='max', verbose=1),\n",
        "            EarlyStopping(monitor='val_auc', patience=5, restore_best_weights=True, verbose=1),\n",
        "            ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6, verbose=1)\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Plot training history with additional metrics\n",
        "    plt.figure(figsize=(16, 10))\n",
        "\n",
        "    # Plot accuracy\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.plot(history.history['accuracy'])\n",
        "    plt.plot(history.history['val_accuracy'])\n",
        "    plt.title('Model Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend(['Train', 'Validation'], loc='lower right')\n",
        "\n",
        "    # Plot loss\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('Model Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend(['Train', 'Validation'], loc='upper right')\n",
        "\n",
        "    # Plot precision\n",
        "    plt.subplot(2, 2, 3)\n",
        "    plt.plot(history.history['precision'])\n",
        "    plt.plot(history.history['val_precision'])\n",
        "    plt.title('Model Precision')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.legend(['Train', 'Validation'], loc='lower right')\n",
        "\n",
        "    # Plot recall\n",
        "    plt.subplot(2, 2, 4)\n",
        "    plt.plot(history.history['recall'])\n",
        "    plt.plot(history.history['val_recall'])\n",
        "    plt.title('Model Recall')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Recall')\n",
        "    plt.legend(['Train', 'Validation'], loc='lower right')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('training_history.png')\n",
        "    plt.close()\n",
        "\n",
        "    # Evaluate the model\n",
        "    y_true, y_pred = evaluate_model(model, test_gen)\n",
        "\n",
        "    # Clean up to save memory\n",
        "    del train_gen, val_gen, test_gen\n",
        "    gc.collect()\n",
        "\n",
        "    print(\"Model training and evaluation completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NN610TydaJEF",
        "outputId": "b1788327-1736-43fc-d688-4c254d7ab113"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set: 513 videos\n",
            "- Shoplifters: 194\n",
            "- Non-shoplifters: 319\n",
            "Validation set: 171 videos\n",
            "- Shoplifters: 65\n",
            "- Non-shoplifters: 106\n",
            "Test set: 171 videos\n",
            "- Shoplifters: 65\n",
            "- Non-shoplifters: 106\n",
            "Using class weights: {0: 0.8050847457627118, 1: 1.3194444444444444}\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)     │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ time_distributed (\u001b[38;5;33mTimeDistributed\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1280\u001b[0m)      │       \u001b[38;5;34m2,257,984\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ time_distributed_1 (\u001b[38;5;33mTimeDistributed\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m1280\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m256\u001b[0m)             │       \u001b[38;5;34m1,442,816\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m256\u001b[0m)             │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ bidirectional_1 (\u001b[38;5;33mBidirectional\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │         \u001b[38;5;34m164,352\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │          \u001b[38;5;34m16,512\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                   │             \u001b[38;5;34m258\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)     │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ time_distributed (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ time_distributed_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)             │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,442,816</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)             │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ bidirectional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">164,352</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">258</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,881,922\u001b[0m (14.81 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,881,922</span> (14.81 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,623,938\u001b[0m (6.19 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,623,938</span> (6.19 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8s/step - accuracy: 0.5379 - auc: 0.5562 - loss: 0.7153 - precision: 0.5379 - recall: 0.5379\n",
            "Epoch 1: val_auc improved from -inf to 0.85780, saving model to best_model.keras\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1488s\u001b[0m 11s/step - accuracy: 0.5378 - auc: 0.5559 - loss: 0.7154 - precision: 0.5378 - recall: 0.5378 - val_accuracy: 0.7602 - val_auc: 0.8578 - val_loss: 0.6420 - val_precision: 0.7602 - val_recall: 0.7602 - learning_rate: 1.0000e-04\n",
            "Epoch 2/10\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8s/step - accuracy: 0.5990 - auc: 0.6423 - loss: 0.6814 - precision: 0.5990 - recall: 0.5990\n",
            "Epoch 2: val_auc improved from 0.85780 to 0.91945, saving model to best_model.keras\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1427s\u001b[0m 11s/step - accuracy: 0.5991 - auc: 0.6424 - loss: 0.6813 - precision: 0.5991 - recall: 0.5991 - val_accuracy: 0.8713 - val_auc: 0.9194 - val_loss: 0.5777 - val_precision: 0.8713 - val_recall: 0.8713 - learning_rate: 1.0000e-04\n",
            "Epoch 3/10\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8s/step - accuracy: 0.6347 - auc: 0.6855 - loss: 0.6419 - precision: 0.6347 - recall: 0.6347\n",
            "Epoch 3: val_auc improved from 0.91945 to 0.92080, saving model to best_model.keras\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1388s\u001b[0m 11s/step - accuracy: 0.6347 - auc: 0.6855 - loss: 0.6418 - precision: 0.6347 - recall: 0.6347 - val_accuracy: 0.8655 - val_auc: 0.9208 - val_loss: 0.4783 - val_precision: 0.8655 - val_recall: 0.8655 - learning_rate: 1.0000e-04\n",
            "Epoch 4/10\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8s/step - accuracy: 0.7678 - auc: 0.8165 - loss: 0.5518 - precision: 0.7678 - recall: 0.7678\n",
            "Epoch 4: val_auc improved from 0.92080 to 0.92093, saving model to best_model.keras\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1414s\u001b[0m 11s/step - accuracy: 0.7677 - auc: 0.8165 - loss: 0.5516 - precision: 0.7677 - recall: 0.7677 - val_accuracy: 0.8363 - val_auc: 0.9209 - val_loss: 0.3674 - val_precision: 0.8363 - val_recall: 0.8363 - learning_rate: 1.0000e-04\n",
            "Epoch 5/10\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8s/step - accuracy: 0.7826 - auc: 0.8431 - loss: 0.5128 - precision: 0.7826 - recall: 0.7826\n",
            "Epoch 5: val_auc improved from 0.92093 to 0.92240, saving model to best_model.keras\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1413s\u001b[0m 11s/step - accuracy: 0.7828 - auc: 0.8432 - loss: 0.5125 - precision: 0.7828 - recall: 0.7828 - val_accuracy: 0.8538 - val_auc: 0.9224 - val_loss: 0.3590 - val_precision: 0.8538 - val_recall: 0.8538 - learning_rate: 1.0000e-04\n",
            "Epoch 6/10\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8s/step - accuracy: 0.8192 - auc: 0.8895 - loss: 0.4047 - precision: 0.8192 - recall: 0.8192\n",
            "Epoch 6: val_auc improved from 0.92240 to 0.94624, saving model to best_model.keras\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1377s\u001b[0m 11s/step - accuracy: 0.8191 - auc: 0.8893 - loss: 0.4050 - precision: 0.8191 - recall: 0.8191 - val_accuracy: 0.8596 - val_auc: 0.9462 - val_loss: 0.3169 - val_precision: 0.8596 - val_recall: 0.8596 - learning_rate: 1.0000e-04\n",
            "Epoch 7/10\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8s/step - accuracy: 0.8419 - auc: 0.8974 - loss: 0.3932 - precision: 0.8419 - recall: 0.8419\n",
            "Epoch 7: val_auc improved from 0.94624 to 0.95363, saving model to best_model.keras\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1436s\u001b[0m 11s/step - accuracy: 0.8421 - auc: 0.8977 - loss: 0.3927 - precision: 0.8421 - recall: 0.8421 - val_accuracy: 0.8947 - val_auc: 0.9536 - val_loss: 0.2915 - val_precision: 0.8947 - val_recall: 0.8947 - learning_rate: 1.0000e-04\n",
            "Epoch 8/10\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8s/step - accuracy: 0.8768 - auc: 0.9387 - loss: 0.2989 - precision: 0.8768 - recall: 0.8768\n",
            "Epoch 8: val_auc did not improve from 0.95363\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1368s\u001b[0m 11s/step - accuracy: 0.8768 - auc: 0.9387 - loss: 0.2988 - precision: 0.8768 - recall: 0.8768 - val_accuracy: 0.8421 - val_auc: 0.9274 - val_loss: 0.3597 - val_precision: 0.8421 - val_recall: 0.8421 - learning_rate: 1.0000e-04\n",
            "Epoch 9/10\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8s/step - accuracy: 0.8773 - auc: 0.9352 - loss: 0.3061 - precision: 0.8773 - recall: 0.8773\n",
            "Epoch 9: val_auc improved from 0.95363 to 0.96011, saving model to best_model.keras\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1428s\u001b[0m 11s/step - accuracy: 0.8774 - auc: 0.9353 - loss: 0.3061 - precision: 0.8774 - recall: 0.8774 - val_accuracy: 0.8772 - val_auc: 0.9601 - val_loss: 0.2754 - val_precision: 0.8772 - val_recall: 0.8772 - learning_rate: 1.0000e-04\n",
            "Epoch 10/10\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8s/step - accuracy: 0.8833 - auc: 0.9442 - loss: 0.2980 - precision: 0.8833 - recall: 0.8833\n",
            "Epoch 10: val_auc improved from 0.96011 to 0.98006, saving model to best_model.keras\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1376s\u001b[0m 11s/step - accuracy: 0.8833 - auc: 0.9441 - loss: 0.2982 - precision: 0.8833 - recall: 0.8833 - val_accuracy: 0.9240 - val_auc: 0.9801 - val_loss: 0.1857 - val_precision: 0.9240 - val_recall: 0.9240 - learning_rate: 1.0000e-04\n",
            "Restoring model weights from the end of the best epoch: 10.\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 40s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 31s/step\n",
            "\n",
            "Test Accuracy: 0.9123\n",
            "\n",
            "Classification Report:\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "Non-Shoplifter       0.93      0.92      0.93       106\n",
            "    Shoplifter       0.88      0.89      0.89        65\n",
            "\n",
            "      accuracy                           0.91       171\n",
            "     macro avg       0.91      0.91      0.91       171\n",
            "  weighted avg       0.91      0.91      0.91       171\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[98  8]\n",
            " [ 7 58]]\n",
            "Model training and evaluation completed!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "boi85CDYZwwm"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}